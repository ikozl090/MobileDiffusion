{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "os.chdir(\"/w/246/ikozlov/csc2231-project/transformer_latent_diffusion\")\n",
    "\n",
    "from tld.train import main\n",
    "from tld.configs import DataDownloadConfig, DataConfig, ModelConfig, TrainConfig, DenoiserConfig, VaeConfig, ClipConfig\n",
    "from accelerate import notebook_launcher\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "from datasets import load_dataset\n",
    "from tld.data import get_text_and_latent_embeddings_hdf5, encode_text, encode_image\n",
    "import clip\n",
    "from diffusers import AutoencoderKL, AutoencoderTiny, StableDiffusionPipeline\n",
    "import numpy as np\n",
    "import huggingface_hub as hgf\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTextModel, AutoTokenizer, CLIPTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Log into W&B and HF \n",
    "os.environ[\"WANDB_API_KEY\"]='57ae32f3ba3cd4a369ec2340fe535fc4ca75e1a7'\n",
    "wandb.login()\n",
    "hgf.login('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')\n",
    "# !wandb login\n",
    "\n",
    "# Initialize device \n",
    "device = 'cuda:0' if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Resolution\n",
    "resolution = DataDownloadConfig.image_size\n",
    "\n",
    "# Latents paths \n",
    "latent_save_path = DataDownloadConfig.latent_save_path\n",
    "image_latent_path = os.path.join(latent_save_path, 'image_latents.npy')\n",
    "text_emb_path = os.path.join(latent_save_path, 'text_encodings.npy')\n",
    "val_emb_path = os.path.join(latent_save_path, 'val_encs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises stored HTTPError, if one occurred.\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        return image\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "    except:\n",
    "        print(f\"Something else went wrong fetching {url}\")\n",
    "        return None\n",
    "\n",
    "def transform(example):\n",
    "    if \"image\" in example: \n",
    "        example['image'] = example['image'].resize((resolution, resolution), Image.BICUBIC)\n",
    "        return example \n",
    "    \n",
    "    if \"image_url\" in example:\n",
    "        example['image'] = load_image_from_url(example['image_url'])\n",
    "        return example\n",
    "        \n",
    "    if \"link\" in example:\n",
    "        example['image'] = load_image_from_url(example['link'])\n",
    "        \n",
    "    if example['image'] is not None:\n",
    "        example['image'] = example['image'].resize((resolution, resolution), Image.BICUBIC)\n",
    "    else:\n",
    "        example['image'] = None\n",
    "    return example\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images, texts = [], []\n",
    "    for item in batch:\n",
    "        image, text = item['image'], item['text']\n",
    "\n",
    "        # Apply the transformation\n",
    "        image = ToTensor()(image)\n",
    "        image_rgb = torch.zeros(3,image.shape[1], image.shape[2])\n",
    "        if image.shape[0] > 3: \n",
    "            image_rgb = image[:3, :, :] \n",
    "        elif image.shape[0] < 3: \n",
    "            image_rgb[0,:,:] = image[0, :, :] \n",
    "            image_rgb[1,:,:] = image[0, :, :] \n",
    "            image_rgb[2,:,:] = image[0, :, :] \n",
    "        \n",
    "        # new_item = dict()\n",
    "        # new_item['image'] = ToTensor()(item['image'])\n",
    "        # new_item['text'] = ToTensor()(item['text'])\n",
    "\n",
    "        images.append(image_rgb)\n",
    "        texts.append(text)\n",
    "\n",
    "\n",
    "    return torch.stack(images), texts\n",
    "\n",
    "def get_text_and_latent_embeddings(dataloader, vae, model, device):\n",
    "    img_encoding_ds = []\n",
    "    text_encoding_ds = []\n",
    "    \n",
    "    for img, label in tqdm(dataloader):\n",
    "            # label_dv = torch.Tensor(label).to(device) \n",
    "            # tokens = tokenizer(label_dv, device=device) \n",
    "            text_tokens = clip.tokenize(label, truncate=True).to(device)\n",
    "            model = model.to(device)\n",
    "            text_encoding = model(text_tokens).cpu().numpy().astype(np.float16)\n",
    "             \n",
    "            #text_encoding = encode_text(label, model, device).cpu().numpy().astype(np.float16)\n",
    "            x = img.to(device).to(torch.float16)\n",
    "            x = x * 2 - 1  # to make it between -1 and 1.\n",
    "            img_encoding = vae.encode(x, return_dict=False)[0].sample().cpu().numpy().astype(np.float16)\n",
    "            \n",
    "            # img_encoding = encode_image(img, vae).cpu().numpy().astype(np.float16)\n",
    "\n",
    "            text_encoding_ds.append(text_encoding)\n",
    "            img_encoding_ds.append(img_encoding)\n",
    "            \n",
    "    return img_encoding_ds, text_encoding_ds\n",
    "\n",
    "def get_img_embeddings(dataloader, vae, device):\n",
    "    img_encoding_ds = []\n",
    "    \n",
    "    print(\"Generating image embeddings...\") \n",
    "    for img, _ in tqdm(dataloader):\n",
    "            #text_encoding = encode_text(label, model, device).cpu().numpy().astype(np.float16)\n",
    "            x = img.to(device).to(VaeConfig.vae_dtype)\n",
    "            x = x * 2 - 1  # to make it between -1 and 1.\n",
    "            vae.to(device)\n",
    "            img_encoding = vae.encode(x, return_dict=False)[0].sample().cpu().detach().numpy().astype(np.float16)\n",
    "            img_encoding_ds.append(img_encoding)\n",
    "            \n",
    "    return img_encoding_ds\n",
    "\n",
    "def get_text_embeddings(model, tokenizer, device, text_list = [], dataloader = None):\n",
    "    text_encoding_ds = []\n",
    "    \n",
    "    print(\"Generating text embeddings...\") \n",
    "    if text_list != []: \n",
    "        for label in tqdm(text_list):\n",
    "                # label_dv = torch.Tensor(label).to(device) \n",
    "                # tokens = tokenizer(label_dv, device=device) \n",
    "                # text_tokens = torch.Tensor(tokenizer(label, truncation=True)).to(device)\n",
    "                \n",
    "                # Tokenize the labels/text\n",
    "                tokens = tokenizer(label, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "                \n",
    "                # Move tokens to the appropriate device\n",
    "                input_ids = tokens['input_ids'].to(device)\n",
    "                attention_mask = tokens['attention_mask'].to(device)\n",
    "                \n",
    "                # Get model output using the prepared tokens\n",
    "                model = model.to(device)\n",
    "                text_encoding = model(input_ids, attention_mask=attention_mask).last_hidden_state.detach().cpu().numpy().astype(np.float16)\n",
    "                \n",
    "                # model = model.to(device)\n",
    "                # text_encoding = model(text_tokens).last_hidden_state.to(\"cpu\").detach().numpy().astype(np.float16)\n",
    "\n",
    "                text_encoding_ds.append(text_encoding)\n",
    "    elif dataloader != None: \n",
    "        for _, label in tqdm(dataloader):\n",
    "                # label_dv = torch.Tensor(label).to(device) \n",
    "                # tokens = tokenizer(label_dv, device=device) \n",
    "                # text_tokens = torch.Tensor(tokenizer(label, truncation=True)).to(device)\n",
    "\n",
    "                # Tokenize the labels/text\n",
    "                tokens = tokenizer(label, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "                \n",
    "                # Move tokens to the appropriate device\n",
    "                input_ids = tokens['input_ids'].to(device)\n",
    "                attention_mask = tokens['attention_mask'].to(device)\n",
    "                \n",
    "                # Get model output using the prepared tokens\n",
    "                model = model.to(device)\n",
    "                text_encoding = model(input_ids, attention_mask=attention_mask).last_hidden_state.detach().cpu().numpy().astype(np.float16)\n",
    "                \n",
    "                # model = model.to(device)\n",
    "                # text_encoding = model(text_tokens).last_hidden_state.to(\"cpu\").detach().numpy().astype(np.float16)\n",
    "\n",
    "                text_encoding_ds.append(text_encoding)\n",
    "            \n",
    "    return text_encoding_ds\n",
    "\n",
    "#######################################\n",
    "#            Load Data Set            #\n",
    "#######################################\n",
    "\n",
    "# Load dataset and apply transforms to training set \n",
    "# dataset_name = 'lambdalabs/pokemon-blip-captions' \n",
    "dataset_name = \"fantasyfish/laion-art\" # \"laion/gpt4v-dataset\" # \"saxon/T2IScoreScore\" # 'valhalla/pokemon-dataset'\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset[\"train\"] = dataset[\"train\"].map(transform)\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda example: example['image'] is not None) # Filter out entries where 'image' is None\n",
    "#dataset.set_format(type='torch', columns=['image', 'text'])\n",
    "\n",
    "# Create DataLoader object \n",
    "train_dataloader = torch.utils.data.DataLoader(dataset[\"train\"], batch_size=DataDownloadConfig.batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "#######################################\n",
    "#    Get Text and Image Encodings     #\n",
    "#######################################\n",
    "\n",
    "text_encoder_hf_path = ClipConfig.clip_model_name # \"openai/clip-vit-base-patch32\" # \"openai/clip-vit-large-patch14\"\n",
    "img_encoder_hf_path = VaeConfig.vae_name # \"madebyollin/taesd\" #\"madebyollin/sdxl-vae-fp16-fix\"\n",
    "\n",
    "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "teacher_text_encoder_hf_path = pretrained_model_name_or_path # \"openai/clip-vit-base-patch32\" # \"openai/clip-vit-large-patch14\"\n",
    "teacher_img_encoder_hf_path =  pretrained_model_name_or_path # \"madebyollin/taesd\" #\"madebyollin/sdxl-vae-fp16-fix\"\n",
    "\n",
    "# Initialize latents path\n",
    "if not os.path.exists(latent_save_path):\n",
    "    os.mkdir(latent_save_path)\n",
    "\n",
    "# Load models \n",
    "# model, preprocess = clip.load(\"ViT-L/14\")\n",
    "model = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"text_encoder\", torch_dtype=ClipConfig.clip_dtype)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# preprocess = CLIPProcessor.from_pretrained(pretrained_model_name_or_path, subfolder=\"\")\n",
    "#vae = AutoencoderKL.from_pretrained(img_encoder_hf_path, torch_dtype=torch.float16)\n",
    "# vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, torch_dtype=VaeConfig.vae_dtype)\n",
    "vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\", torch_dtype=VaeConfig.vae_dtype)\n",
    "# vae = vae.to('cuda')\n",
    "# model.to('cuda')\n",
    "\n",
    "# Generate embeddings \n",
    "# img_encodings, text_encodings = get_text_and_latent_embeddings(train_dataloader, vae, model, device)\n",
    "img_encodings = get_img_embeddings(train_dataloader, vae, device) \n",
    "text_encodings = get_text_embeddings(model, tokenizer ,device, dataloader = train_dataloader)\n",
    "\n",
    "# Save latents to path \n",
    "np.save(image_latent_path, np.concatenate(img_encodings, axis=0))\n",
    "np.save(text_emb_path, np.concatenate(text_encodings, axis=0))\n",
    "\n",
    "# Load teacher models \n",
    "# teacher_text_embedding = \n",
    "\n",
    "#######################################\n",
    "#  Save Validation Prompt Encodings   #\n",
    "#######################################\n",
    "\n",
    "creature_descriptions = [\n",
    "    \"A drawing of a small, blue aquatic creature with a fin on its head and a light blue tail.\",\n",
    "    \"A picture of a fiery orange and red mythical dragon-like figure, with smoke billowing from its nostrils.\",\n",
    "    \"A cartoon image of a character that looks like a yellow sunflower with a smiling face in the center.\",\n",
    "    \"An illustration of a rock-like creature, gray and rugged, with crystals emerging from its back.\",\n",
    "    \"A sketch of a ghostly figure, transparent and white, with glowing red eyes and ethereal trails.\",\n",
    "    \"A drawing of a cute, furry, brown bear cub-like character, with large, round ears and a small nose.\",\n",
    "    \"An image of an electric-type creature, bright yellow with black stripes, radiating energy.\",\n",
    "    \"A picture of an ice-like character, resembling a small, crystalline snowflake with a shimmering, icy body.\"\n",
    "]\n",
    "\n",
    "np.save(val_emb_path, get_text_embeddings(model, tokenizer, device, text_list=creature_descriptions))\n",
    "print(\"Done with conversion to latents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id='' #@param {type:\"string\"}\n",
    "n_epoch=40 #@param {type:\"integer\"}\n",
    "\n",
    "\n",
    "data_config = DataConfig(latent_path=image_latent_path,\n",
    "                        text_emb_path=text_emb_path,\n",
    "                        val_path=val_emb_path)\n",
    "\n",
    "denoiser_config = DenoiserConfig(image_size=int(resolution/8))\n",
    "\n",
    "model_cfg = ModelConfig(\n",
    "    data_config=data_config,\n",
    "    denoiser_config=denoiser_config,\n",
    "    train_config=TrainConfig(),\n",
    ")\n",
    "\n",
    "notebook_launcher(main, (model_cfg,), num_processes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tld)",
   "language": "python",
   "name": "tld"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
